{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiConvLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP8VkrozqBqb8dLh6ChPqPc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ditsuhi/ExploratoryAnalysis_FeatureSelection/blob/main/BiConvLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVZVoVBz04Ol"
      },
      "outputs": [],
      "source": [
        "# import all required libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from time import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow import keras\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import ConvLSTM2D, BatchNormalization, Dropout, Bidirectional, Conv2D "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the dataset can be found at the following link: https://doi.org/10.5281/zenodo.6497108. \n",
        "# the path provided below can be changed depending your data location.\n",
        "\n",
        "datafr_2019 = pd.read_csv('/content/Madrid_wind_2019.csv', index_col='Unnamed: 0')\n",
        "datafr_2020 = pd.read_csv('/content/Madrid_wind_2020.csv', index_col='Unnamed: 0')"
      ],
      "metadata": {
        "id": "r71E87s41CUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this part depends on the selected features, extracted after mutual information and mRMR implementation.\n",
        "# here is one example with 16 features selected.\n",
        "\n",
        "datafr_new_2019=datafr_2019[['NO2', 'intensidad', 'ocupacion', 'windSpeed', ' Pressure', ' SolarRad',\n",
        "       ' Temperature', ' Humidity', 'windDir_Categ_east', 'windDir_Categ_north', 'windDir_Categ_northeast',\n",
        "       'windDir_Categ_northwest', 'windDir_Categ_south',  'windDir_Categ_southeast', 'windDir_Categ_southwest',\n",
        "       'windDir_Categ_west'\n",
        "       ]]\n",
        "     \n",
        "\n",
        "datafr_new_2020=datafr_2020[['NO2', 'intensidad', 'ocupacion', 'windSpeed', 'Pressure', 'SolarRad',\n",
        "       'Temp', 'Humidity', 'windDir_Categ_east', 'windDir_Categ_north', 'windDir_Categ_northeast',\n",
        "       'windDir_Categ_northwest', 'windDir_Categ_south',  'windDir_Categ_southeast', 'windDir_Categ_southwest',\n",
        "       'windDir_Categ_west'\n",
        "       ]]"
      ],
      "metadata": {
        "id": "PQ0I-bWs1Nz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert dataframes to numpy array and reshape them\n",
        "data_np_2019 = np.asarray(datafr_new_2019)\n",
        "data_np_2020 = np.asarray(datafr_new_2020)\n",
        "data_np_2019_resh = data_np_2019.reshape(-1, 340, 16)\n",
        "data_np_2020_resh = data_np_2020.reshape(-1, 340,  16)\n",
        "\n",
        "# define training, validation and testing sets\n",
        "data_2019_train = data_np_2019_resh[:, :, :]\n",
        "data_2020_val = data_np_2020_resh[0:2184, :, :]\n",
        "data_2020_test = data_np_2020_resh[2184::, :, :]"
      ],
      "metadata": {
        "id": "9dcSYXzu1nWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset to X and y (dependent and independent)\n",
        "\n",
        "def split_sequence(sequence, time_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequence)):\n",
        "   \n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + 6\n",
        "    \n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif end_ix+time_steps > len(sequence)-1:\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern    \n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix: end_ix+time_steps]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn np.array(X), np.array(y)\n",
        " \n",
        "\n",
        "\n",
        "# choose a number of time steps and define dependent and independent sets.\n",
        "time_steps = 6\n",
        "X_train_notNorm, y_train = split_sequence(data_2019_train, time_steps)\n",
        "X_val_notNorm, y_val = split_sequence(data_2020_val, time_steps)\n",
        "X_test_notNorm, y_test = split_sequence(data_2020_test, time_steps)\n"
      ],
      "metadata": {
        "id": "oUGfGNjH10xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to normalise train data using MinMaxScaler\n",
        "\n",
        "number_selected_columns =16\n",
        "scaler = MinMaxScaler(feature_range=(0, 1), copy = False)\n",
        "X_train_Normalised = X_train_notNorm.reshape(-1, number_selected_columns)\n",
        "X_val_Normalised = X_val_notNorm.reshape(-1, number_selected_columns)\n",
        "X_test_Normalised = X_test_notNorm.reshape(-1, number_selected_columns)\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train_Normalised)\n",
        "X_val_scaled = scaler.transform(X_val_Normalised)\n",
        "X_test_scaled = scaler.transform(X_test_Normalised)\n",
        "\n",
        "X_train = X_train_scaled.reshape(X_train_notNorm.shape[0], X_train_notNorm.shape[1], X_train_notNorm.shape[2], X_train_notNorm.shape[3])\n",
        "X_val = X_val_scaled.reshape(X_val_notNorm.shape[0], X_val_notNorm.shape[1], X_val_notNorm.shape[2], X_val_notNorm.shape[3])\n",
        "X_test = X_test_scaled.reshape(X_test_notNorm.shape[0], X_test_notNorm.shape[1], X_test_notNorm.shape[2], X_test_notNorm.shape[3])"
      ],
      "metadata": {
        "id": "uZWaGuFT15U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to reshape data corresponding to the input data of BiConvLSTM method.\n",
        "\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 20, 17*number_selected_columns, 1))\n",
        "y_train_reshaped = y_train.reshape((y_train.shape[0], y_train.shape[1], 20, 17*number_selected_columns, 1))\n",
        "X_val_reshaped = X_val.reshape((X_val.shape[0], X_val.shape[1], 20, 17*number_selected_columns, 1))\n",
        "y_val_reshaped = y_val.reshape(y_val.shape[0], y_val.shape[1], 20, 17*number_selected_columns, 1)\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 20, 17*number_selected_columns, 1))\n",
        "y_test_reshaped = y_test.reshape(y_test.shape[0], y_test.shape[1], 20, 17*number_selected_columns, 1)"
      ],
      "metadata": {
        "id": "3LyQ8Msl19w8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_reshaped.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiyYJS7rCXfj",
        "outputId": "2cfdd31d-90ba-4f4a-d2c3-1c0ca7f98a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2171, 6, 20, 272, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the architecture of the proposed model.\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "def create_model(number_selected_columns=16, optimizer=opt, kernel_size=(3, 3), filters=16, merge_mode=\"concat\", dropout_rate=0.2):\n",
        "    \n",
        "    model = Sequential()    \n",
        "    model.add(Bidirectional(ConvLSTM2D(input_shape=(None, 20, 17*number_selected_columns, 1),  filters=filters,  kernel_size=kernel_size, padding=\"same\", return_sequences=True), merge_mode=merge_mode))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout_rate))   \n",
        "    model.add(Bidirectional(ConvLSTM2D(filters=filters, kernel_size=kernel_size, padding=\"same\", return_sequences=True), merge_mode=merge_mode))    \n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout_rate)) \n",
        "    model.add(Bidirectional(ConvLSTM2D(filters=filters,  kernel_size=kernel_size, padding=\"same\", return_sequences=True), merge_mode=merge_mode))     \n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout_rate))            \n",
        "    model.add(Conv2D(filters=1, kernel_size=(1, 1), \n",
        "                activation='elu',\n",
        "                padding='same', data_format='channels_last'))\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "    model.build(input_shape=(None,6,  20, 17*number_selected_columns, 1))    \n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "metadata": {
        "id": "fOozHbcs2PM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the model\n",
        "\n",
        "model = create_model()\n",
        "start = time()\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=2)\n",
        "final_model = model.fit(X_train_reshaped, y_train_reshaped, epochs=50, verbose=2, validation_data=(X_val_reshaped, y_val_reshaped),  callbacks=[early_stopping])\n",
        "print(f'Time taken to run: {time() - start} seconds')"
      ],
      "metadata": {
        "id": "D3idPPjeV5_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        " \n",
        "# perform prediction\n",
        "yhat = model.predict(X_test_reshaped, verbose=1)\n",
        "# reshape the predicted results and the test data for further evaluation\n",
        "yhat_reshaped = yhat.reshape(-1,20*17*16)\n",
        "y_test_reshaped=  y_test_reshaped.reshape(-1,20*17*16)\n",
        "# calculate RMSE and MAE\n",
        "rmse = mean_squared_error(yhat_reshaped, y_test_reshaped, squared=False)\n",
        "mae = mean_absolute_error(yhat_reshaped, y_test_reshaped)\n",
        "print('Test Score: %.2f RMSE' % (rmse))\n",
        "print('Test Score: %.2f MAE' % (mae))"
      ],
      "metadata": {
        "id": "KLDTs28acNwd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}