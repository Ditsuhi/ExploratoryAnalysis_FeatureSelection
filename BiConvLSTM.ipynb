{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ditsuhi/ExploratoryAnalysis_FeatureSelection/blob/main/Copy_of_BiConvLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sVZVoVBz04Ol"
      },
      "outputs": [],
      "source": [
        "# import all required libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from time import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow import keras\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import ConvLSTM2D, BatchNormalization, Dropout, Bidirectional, Conv2D "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r71E87s41CUm"
      },
      "outputs": [],
      "source": [
        "# the dataset can be found at the following link: https://doi.org/10.5281/zenodo.6497108. \n",
        "# the path provided below can be changed depending your data location.\n",
        "\n",
        "datafr_2019 = pd.read_csv('/content/Madrid_wind_2019.csv')\n",
        "datafr_2020 = pd.read_csv('/content/Madrid_wind_2020.csv', index_col='Unnamed: 0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w7hO2ucEQXJ",
        "outputId": "f9d1f7db-45ee-41ef-ccec-577f44c08001"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['NO2', 'windSpeed', 'Temp', 'Humidity', 'Pressure', 'SolarRad',\n",
              "       'intensidad', 'ocupacion', 'carga', 'vmed', 'v_comp', 'u_comp',\n",
              "       'windDir_Categ_east', 'windDir_Categ_north', 'windDir_Categ_northeast',\n",
              "       'windDir_Categ_northwest', 'windDir_Categ_south',\n",
              "       'windDir_Categ_southeast', 'windDir_Categ_southwest',\n",
              "       'windDir_Categ_west'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "datafr_2020.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1gmq_3aaq9m",
        "outputId": "9bb8d6a4-6801-4f49-ce8b-8b56624d2b58"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['NO2', 'windSpeed', 'Temp', 'Humidity', 'Pressure', 'SolarRad',\n",
              "       'intensidad', 'ocupacion', 'carga', 'vmed', 'v_comp', 'u_comp',\n",
              "       'windDir_Categ_east', 'windDir_Categ_north', 'windDir_Categ_northeast',\n",
              "       'windDir_Categ_northwest', 'windDir_Categ_south',\n",
              "       'windDir_Categ_southeast', 'windDir_Categ_southwest',\n",
              "       'windDir_Categ_west'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "datafr_2019.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PQ0I-bWs1Nz0"
      },
      "outputs": [],
      "source": [
        "# this part depends on the selected features, extracted after mutual information and mRMR implementation.\n",
        "# Below are the features for each scenario after implementing feature selection techniques\n",
        "\n",
        "\n",
        "\n",
        "# # Scenario_First_All_Features\n",
        "# datafr_new_2019=datafr_2019[['NO2', 'windSpeed', 'Temp', 'Humidity', 'Pressure', 'SolarRad',\n",
        "#        'intensidad', 'ocupacion', 'carga', 'vmed', 'windDir_Categ_east', 'windDir_Categ_north', 'windDir_Categ_northeast',\n",
        "#        'windDir_Categ_northwest', 'windDir_Categ_south',\n",
        "#        'windDir_Categ_southeast', 'windDir_Categ_southwest',\n",
        "#        'windDir_Categ_west']\n",
        "#       ]\n",
        "     \n",
        "\n",
        "# datafr_new_2020=datafr_2020[['NO2', 'windSpeed', 'Temp', 'Humidity', 'Pressure', 'SolarRad',\n",
        "#        'intensidad', 'ocupacion', 'carga', 'vmed', 'windDir_Categ_east', 'windDir_Categ_north', 'windDir_Categ_northeast',\n",
        "#        'windDir_Categ_northwest', 'windDir_Categ_south',\n",
        "#        'windDir_Categ_southeast', 'windDir_Categ_southwest',\n",
        "#        'windDir_Categ_west']]\n",
        "\n",
        "\n",
        "# # Scenario_Second_All_Features\n",
        "# datafr_new_2019=datafr_2019[['NO2', 'windSpeed', 'Temp', 'Humidity', 'Pressure', 'SolarRad',\n",
        "#        'intensidad', 'ocupacion', 'carga', 'vmed', 'v_comp', 'u_comp']]\n",
        "     \n",
        "\n",
        "# datafr_new_2020=datafr_2020[['NO2', 'windSpeed', 'Temp', 'Humidity', 'Pressure', 'SolarRad',\n",
        "#        'intensidad', 'ocupacion', 'carga', 'vmed', 'v_comp', 'u_comp']]\n",
        "\n",
        "\n",
        "\n",
        "# # Scenario_First_MI \n",
        "# datafr_new_2019=datafr_2019[['NO2', 'windSpeed',  'Pressure', 'intensidad', 'ocupacion', 'carga', 'vmed']]\n",
        "     \n",
        "# datafr_new_2020=datafr_2020[['NO2', 'windSpeed',  'Pressure', 'intensidad', 'ocupacion', 'carga', 'vmed']]\n",
        "\n",
        "\n",
        "\n",
        "# # Scenario_Second_MI \n",
        "# datafr_new_2019=datafr_2019[['NO2', 'windSpeed',  'Pressure', 'intensidad', 'ocupacion', 'carga', 'vmed', 'v_comp', 'u_comp']]\n",
        "     \n",
        "# datafr_new_2020=datafr_2020[['NO2', 'windSpeed',  'Pressure', 'intensidad', 'ocupacion', 'carga', 'vmed', 'v_comp', 'u_comp']]\n",
        "\n",
        "\n",
        "# # Scenario_First_mRMR (K=8). Here, the characteristics are written in descending order of importance of predicting nitrogen dioxide. \n",
        "# # For example, if K=3, we should extract the first 4 features since NO2 is the first feature and the other 3 are the most relevant features.\n",
        "# #'carga', 'windDir_Categ_northwest', 'Pressure', 'windSpeed', 'vmed', 'ocupacion', 'windDir_Categ_north', 'intensidad'\n",
        "# datafr_new_2019=datafr_2019[['NO2', 'carga', 'windDir_Categ_northwest', 'Pressure']]\n",
        "     \n",
        "# datafr_new_2020=datafr_2020[['NO2', 'carga', 'windDir_Categ_northwest', 'Pressure']]\n",
        "\n",
        "\n",
        "# Scenario_Second_mRMR (K=8). Here, the characteristics are written in descending order of importance of predicting nitrogen dioxide. \n",
        "# For example, if K=3, we should extract the first 4 features since NO2 is the first feature and the other 3 are the most relevant features.\n",
        "# 'NO2', 'carga', 'Pressure', 'windSpeed', 'vmed', 'ocupacion', 'SolarRad', 'intensidad', 'Temp'\n",
        "datafr_new_2019=datafr_2019[['NO2', 'carga', 'Pressure', 'windSpeed', 'vmed', 'ocupacion', 'SolarRad', 'intensidad', 'Temp']]\n",
        "     \n",
        "datafr_new_2020=datafr_2020[['NO2', 'carga', 'Pressure', 'windSpeed', 'vmed', 'ocupacion', 'SolarRad', 'intensidad', 'Temp']]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqjTClEgclme",
        "outputId": "a7469d65-74bb-4067-809c-67e0b3260034"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "len(datafr_new_2020.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9dcSYXzu1nWV"
      },
      "outputs": [],
      "source": [
        "# convert dataframes to numpy array and reshape them. \n",
        "\n",
        "# number of selected features\n",
        "number_selected_columns =9\n",
        "\n",
        "data_np_2019 = np.asarray(datafr_new_2019)\n",
        "data_np_2020 = np.asarray(datafr_new_2020)\n",
        "data_np_2019_resh = data_np_2019.reshape(-1, 340, number_selected_columns)\n",
        "data_np_2020_resh = data_np_2020.reshape(-1, 340, number_selected_columns)\n",
        "\n",
        "# define training, validation and testing sets\n",
        "data_2019_train = data_np_2019_resh[:, :, :]\n",
        "data_2020_val = data_np_2020_resh[0:2184, :, :]\n",
        "data_2020_test = data_np_2020_resh[2184::, :, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oUGfGNjH10xl"
      },
      "outputs": [],
      "source": [
        "# split dataset to X and y (dependent and independent)\n",
        "\n",
        "def split_sequence(sequence, time_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequence)):\n",
        "   \n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + 6\n",
        "    \n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif end_ix+time_steps > len(sequence)-1:\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern    \n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix: end_ix+time_steps]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn np.array(X), np.array(y)\n",
        " \n",
        "\n",
        "\n",
        "# choose a number of time steps and define dependent and independent sets.\n",
        "time_steps = 6\n",
        "X_train_notNorm, y_train = split_sequence(data_2019_train, time_steps)\n",
        "X_val_notNorm, y_val = split_sequence(data_2020_val, time_steps)\n",
        "X_test_notNorm, y_test = split_sequence(data_2020_test, time_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uZWaGuFT15U8"
      },
      "outputs": [],
      "source": [
        "# to normalise train data using MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1), copy = False)\n",
        "X_train_Normalised = X_train_notNorm.reshape(-1, number_selected_columns)\n",
        "X_val_Normalised = X_val_notNorm.reshape(-1, number_selected_columns)\n",
        "X_test_Normalised = X_test_notNorm.reshape(-1, number_selected_columns)\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train_Normalised)\n",
        "X_val_scaled = scaler.transform(X_val_Normalised)\n",
        "X_test_scaled = scaler.transform(X_test_Normalised)\n",
        "\n",
        "X_train = X_train_scaled.reshape(X_train_notNorm.shape[0], X_train_notNorm.shape[1], X_train_notNorm.shape[2], X_train_notNorm.shape[3])\n",
        "X_val = X_val_scaled.reshape(X_val_notNorm.shape[0], X_val_notNorm.shape[1], X_val_notNorm.shape[2], X_val_notNorm.shape[3])\n",
        "X_test = X_test_scaled.reshape(X_test_notNorm.shape[0], X_test_notNorm.shape[1], X_test_notNorm.shape[2], X_test_notNorm.shape[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3LyQ8Msl19w8"
      },
      "outputs": [],
      "source": [
        "# to reshape data corresponding to the input data of BiConvLSTM method.\n",
        "\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 20, 17*number_selected_columns, 1))\n",
        "y_train_reshaped = y_train.reshape((y_train.shape[0], y_train.shape[1], 20, 17*number_selected_columns, 1))\n",
        "X_val_reshaped = X_val.reshape((X_val.shape[0], X_val.shape[1], 20, 17*number_selected_columns, 1))\n",
        "y_val_reshaped = y_val.reshape(y_val.shape[0], y_val.shape[1], 20, 17*number_selected_columns, 1)\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 20, 17*number_selected_columns, 1))\n",
        "y_test_reshaped = y_test.reshape(y_test.shape[0], y_test.shape[1], 20, 17*number_selected_columns, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiyYJS7rCXfj",
        "outputId": "dea31084-ce62-4529-8d28-7e19c26a56b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2171, 6, 20, 153, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "y_test_reshaped.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9192ff91-f4d1-4f2c-f6ed-ef2e2cb2cf5c",
        "id": "-770Pg2GjlJh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model from disk\n"
          ]
        }
      ],
      "source": [
        "# run trained models using .json and .h5 files for each subsets (extracted features for each scenario implementing mutual information and mRMR)\n",
        "\n",
        "from keras.models import model_from_json\n",
        "json_file = open('/content/Scenario_Second_mRMR_(K=8).json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"/content/Scenario_Second_mRMR_(K=8).h5\")\n",
        "print(\"Loaded model from disk\")\n",
        " \n",
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss='mse')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3e7bbe4-fde4-44d7-d94f-14fa812004e7",
        "id": "enQi7Rk5jnjy"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68/68 [==============================] - 15s 165ms/step\n",
            "Test Score: 31.80 RMSE\n",
            "Test Score: 21.77 MAE\n"
          ]
        }
      ],
      "source": [
        "# evaluate the model\n",
        " \n",
        "# perform prediction\n",
        "yhat = loaded_model.predict(X_test_reshaped, verbose=1)\n",
        "# reshape the predicted results and the test data for further evaluation\n",
        "yhat_reshaped = yhat.reshape(-1,20*17*number_selected_columns)\n",
        "y_test_reshaped=  y_test_reshaped.reshape(-1,20*17*number_selected_columns)\n",
        "# calculate RMSE and MAE\n",
        "rmse = mean_squared_error(yhat_reshaped, y_test_reshaped, squared=False)\n",
        "mae = mean_absolute_error(yhat_reshaped, y_test_reshaped)\n",
        "print('Test Score: %.2f RMSE' % (rmse))\n",
        "print('Test Score: %.2f MAE' % (mae))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UHC1a3NvktEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the implemented model with complete architecture. The model can run separately without the trained files. It should be noted that in this case, the results may vary slightly due to the existing randomness in stochastic machine learning algorithms, and this may also be caused by the development environment, considering different software versions and CPU types."
      ],
      "metadata": {
        "id": "LrDNd1OMkrGl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fOozHbcs2PM8"
      },
      "outputs": [],
      "source": [
        "# define the architecture of the proposed model.\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "def create_model(number_selected_columns=number_selected_columns, optimizer=opt, kernel_size=(3, 3), filters=16, merge_mode=\"concat\", dropout_rate=0.2):\n",
        "    \n",
        "    model = Sequential()    \n",
        "    model.add(Bidirectional(ConvLSTM2D(input_shape=(None, 20, 17*number_selected_columns, 1),  filters=filters,  kernel_size=kernel_size, padding=\"same\", return_sequences=True), merge_mode=merge_mode))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout_rate))   \n",
        "    model.add(Bidirectional(ConvLSTM2D(filters=filters, kernel_size=kernel_size, padding=\"same\", return_sequences=True), merge_mode=merge_mode))    \n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout_rate)) \n",
        "    model.add(Bidirectional(ConvLSTM2D(filters=filters,  kernel_size=kernel_size, padding=\"same\", return_sequences=True), merge_mode=merge_mode))     \n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout_rate))            \n",
        "    model.add(Conv2D(filters=1, kernel_size=(1, 1), \n",
        "                activation='elu',\n",
        "                padding='same', data_format='channels_last'))\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "    model.build(input_shape=(None,6,  20, 17*number_selected_columns, 1))    \n",
        "    print(model.summary())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3idPPjeV5_u",
        "outputId": "09251133-24e1-4fcd-da3d-26214860f65a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_3 (Bidirectio  (None, 6, 20, 153, 32)   19712     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 6, 20, 153, 32)   128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 6, 20, 153, 32)    0         \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirectio  (None, 6, 20, 153, 32)   55424     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 6, 20, 153, 32)   128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 6, 20, 153, 32)    0         \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirectio  (None, 6, 20, 153, 32)   55424     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 6, 20, 153, 32)   128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 6, 20, 153, 32)    0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 6, 20, 153, 1)     33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 130,977\n",
            "Trainable params: 130,785\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "136/136 - 114s - loss: 97362.6406 - val_loss: 67139.6797 - 114s/epoch - 835ms/step\n",
            "Epoch 2/50\n",
            "136/136 - 101s - loss: 17792.8145 - val_loss: 19353.5762 - 101s/epoch - 744ms/step\n",
            "Epoch 3/50\n",
            "136/136 - 102s - loss: 7155.3955 - val_loss: 8696.0625 - 102s/epoch - 754ms/step\n",
            "Epoch 4/50\n",
            "136/136 - 103s - loss: 5713.5151 - val_loss: 5938.5522 - 103s/epoch - 755ms/step\n",
            "Epoch 5/50\n",
            "136/136 - 103s - loss: 4960.2749 - val_loss: 5708.2510 - 103s/epoch - 755ms/step\n",
            "Epoch 6/50\n",
            "136/136 - 103s - loss: 4692.0815 - val_loss: 6572.6709 - 103s/epoch - 755ms/step\n",
            "Epoch 7/50\n",
            "136/136 - 103s - loss: 4395.2095 - val_loss: 4259.6602 - 103s/epoch - 754ms/step\n",
            "Epoch 8/50\n",
            "136/136 - 103s - loss: 4267.9678 - val_loss: 5594.6772 - 103s/epoch - 755ms/step\n",
            "Epoch 9/50\n",
            "136/136 - 103s - loss: 4032.5227 - val_loss: 5855.7271 - 103s/epoch - 755ms/step\n",
            "Epoch 10/50\n",
            "136/136 - 103s - loss: 4111.7349 - val_loss: 4593.7725 - 103s/epoch - 755ms/step\n",
            "Epoch 11/50\n",
            "136/136 - 103s - loss: 3945.7812 - val_loss: 5575.4282 - 103s/epoch - 755ms/step\n",
            "Epoch 12/50\n",
            "136/136 - 103s - loss: 3778.6079 - val_loss: 4551.3604 - 103s/epoch - 755ms/step\n",
            "Epoch 12: early stopping\n",
            "Time taken to run: 1242.0708723068237 seconds\n"
          ]
        }
      ],
      "source": [
        "# run the model\n",
        "\n",
        "model = create_model()\n",
        "start = time()\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=2)\n",
        "final_model = model.fit(X_train_reshaped, y_train_reshaped, epochs=50, verbose=2, validation_data=(X_val_reshaped, y_val_reshaped),  callbacks=[early_stopping])\n",
        "print(f'Time taken to run: {time() - start} seconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OTKRPui1zAgb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2e81439-b982-48b3-8073-a94b9e69e64c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68/68 [==============================] - 14s 175ms/step\n",
            "Test Score: 30.66 RMSE\n",
            "Test Score: 19.39 MAE\n"
          ]
        }
      ],
      "source": [
        "# evaluate the model\n",
        " \n",
        "# perform prediction\n",
        "yhat = model.predict(X_test_reshaped, verbose=1)\n",
        "# reshape the predicted results and the test data for further evaluation\n",
        "yhat_reshaped = yhat.reshape(-1,20*17*number_selected_columns)\n",
        "y_test_reshaped=  y_test_reshaped.reshape(-1,20*17*number_selected_columns)\n",
        "# calculate RMSE and MAE\n",
        "rmse = mean_squared_error(yhat_reshaped, y_test_reshaped, squared=False)\n",
        "mae = mean_absolute_error(yhat_reshaped, y_test_reshaped)\n",
        "print('Test Score: %.2f RMSE' % (rmse))\n",
        "print('Test Score: %.2f MAE' % (mae))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kirh4qOaS7qn",
        "outputId": "51f5c68d-f2f3-4061-ca52-600f8d049ee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to disk\n",
            "Loaded model from disk\n"
          ]
        }
      ],
      "source": [
        "# here is an example of saving and loading trained files\n",
        "\n",
        "from keras.models import model_from_json\n",
        "\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"Scenario_Second_mRMR_(K=2).json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"Scenario_Second_mRMR_(K=2).h5\")\n",
        "print(\"Saved model to disk\")\n",
        " \n",
        "\n",
        "# load json and create model\n",
        "json_file = open('Scenario_Second_mRMR_(K=2).json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"Scenario_Second_mRMR_(K=2).h5\")\n",
        "print(\"Loaded model from disk\")\n",
        " \n",
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(optimizer=opt, loss='mse')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Copy of BiConvLSTM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOLHRhfz1ABVW0IbsFcWJxM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
